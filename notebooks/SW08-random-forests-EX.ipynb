{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e1c31c4-faf7-4f80-a6f1-6a22a410570f",
   "metadata": {},
   "source": [
    "# **SW08: Random forests**\n",
    "\n",
    "Random forests are an ensemble learning method that combines multiple decision trees to improve predictive accuracy and reduce overfitting. Each tree in a random forest is trained on a random subset of the data (using bootstrapping) and only a random subset of features at each split, which introduces variety among the trees. During prediction, the random forest aggregates the output of all trees, either by majority voting (for classification) or averaging (for regression). This approach makes random forests robust, accurate, and less prone to overfitting compared to individual decision trees.\n",
    "\n",
    "In this tutorial, we will once more use the iris dataset to classify the different species of iris flowers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec53ed0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Setup**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Some Jupyter magic for nicer output\n",
    "%config InlineBackend.figure_formats = [\"svg\"]   # Enable vectorized graphics\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Adjust the default settings for plots\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import ml\n",
    "ml.setup_plotting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a248ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "data = load_iris(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "display(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40faa57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Basic example**\n",
    "\n",
    "Let's apply a random forest directly to predict how it performs on the iris dataset.\n",
    "\n",
    "The relevant scikit-learn classes are \n",
    "[RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) \n",
    "and \n",
    "[RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da03db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100,       # Number of decision trees (size of ensemble)\n",
    "    max_depth=None,         # Depth of each tree (None means \"unlimited\")\n",
    "    random_state=0,         # Random seed for reproducibility\n",
    "    criterion='gini',       # How to measure the quality of a split\n",
    "    min_samples_split=2,    # Number of samples required to split a node\n",
    ")\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "accuracy_score(y, y_pred)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd129d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 1    ###\n",
    "########################\n",
    "\n",
    "# Part 1:\n",
    "# -------\n",
    "# Play around with the parameters of the RandomForestClassifier. What happens\n",
    "# if you increase the number of trees? What happens if you increase the depth\n",
    "# of the trees? ...\n",
    "\n",
    "# Part 2:\n",
    "# -------\n",
    "# Remember: It is recommended to assess the model's performance on a separate\n",
    "# test set, rather than the training set. Can you update the code such that it \n",
    "# uses a train-test split?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277ae1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Feature importance**\n",
    "\n",
    "Feature importance in random forests is a measure of how valuable each feature is in predicting the target outcome. Random forests calculate feature importance by evaluating how much each feature contributes to the model's accuracy, typically based on how often a feature is used to split nodes across all trees and the quality of these splits. Features that consistently improve prediction accuracy or reduce node impurity (e.g., Gini impurity or entropy) are assigned higher importance scores. This helps identify which features are most influential in the model, providing insights into the underlying patterns in the data.\n",
    "\n",
    "We can easily extract the feature importances from a trained random forest\n",
    "classifier by looking at the attribute `feature_importances_`. We can then\n",
    "visualize the importances in a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9447d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 2    ###\n",
    "########################\n",
    "\n",
    "# Train a random forest classifier on the entire iris dataset. Then,\n",
    "# visualize the feature importances using a bar plot. Which feature(s) \n",
    "# are the most important according to the random forest?\n",
    "\n",
    "# Train a random forest classifier with the best hyperparameters\n",
    "clf = RandomForestClassifier(n_estimators=100, \n",
    "                             max_depth=4, \n",
    "                             random_state=0)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba256dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Decision boundaries**\n",
    "\n",
    "In a previous tutorial (on decision trees), we have used the function \n",
    "plot_decision_boundary() to visualize the decision boundary of a classifier \n",
    "using two features. Let's use this function again to visualize the decision\n",
    "boundary, this time of a kNN classifier. For visualization purposes, we will\n",
    "only use the first two features of the iris dataset.\n",
    "\n",
    "\n",
    "```python\n",
    "def plot_decision_boundary(clf, X, y, \n",
    "                          n_steps=1000, \n",
    "                          data=None, \n",
    "                          ax=None):\n",
    "    \"\"\"\n",
    "    Visualize the decision boundary of an arbitrary classifier.\n",
    "\n",
    "    clf:  The classifier to plot.\n",
    "    X:    The features of the dataset.\n",
    "    y:    The labels of the dataset.\n",
    "    n_steps: Parameter controlling the resolution of plot.\n",
    "    ax:   (optional) The axis to plot on. If None, a new figure is created.\n",
    "    data: (optional) Data structure provided by sklearn.datasets.load_iris().\n",
    "    \"\"\"\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa07723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml import plot_decision_boundary\n",
    "\n",
    "# Note: Choose your own two features here... According to the feature\n",
    "# importances reported above, the separation is best for petal length and\n",
    "# petal width. (We pick here the more difficult case, to see that a random\n",
    "# forest is able to learn relatively complex decision boundaries.)\n",
    "features = [\"sepal length (cm)\", \"sepal width (cm)\"]\n",
    "X_2d = X[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 3    ###\n",
    "########################\n",
    "\n",
    "#Â Use the function plot_decision_boundary() to look how the decision boundary\n",
    "# changes when using different configurations of the RandomForestClassifier.\n",
    "# Just play around with the parameters and see what happens.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ml-hs24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
