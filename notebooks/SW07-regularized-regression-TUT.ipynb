{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ae0ad9-f22c-4783-b782-b5d5358132b6",
   "metadata": {},
   "source": [
    "# **SW07: Regularized Regression Methods**\n",
    "\n",
    "Regularization can help to reduce overfitting in regression or logistic regression models.\n",
    "In this notebook, we briefly explore the effect of regularization.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Some Jupyter magic for nicer output\n",
    "%config InlineBackend.figure_formats = [\"svg\"]   # Enable vectorized graphics\n",
    "\n",
    "# Adjust the default settings for plots\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import ml\n",
    "ml.setup_plotting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e6a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate some data for regression\n",
    "n_samples = 50\n",
    "noise = 0.3\n",
    "seed = 42\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "def generate_data(n_samples, noise, seed, fun=None):\n",
    "    np.random.seed(seed)\n",
    "    x = np.sort(np.random.rand(n_samples))\n",
    "    y = fun(x) + np.random.randn(n_samples) * noise\n",
    "    X = x[:, np.newaxis]\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = generate_data(n_samples, noise, seed=42, fun=f)\n",
    "X_test, y_test = generate_data(n_samples*10, noise, seed=43, fun=f)\n",
    "\n",
    "# Let's also visualize the data\n",
    "X_gt = np.linspace(X_train.min(), X_train.max(), 1000)\n",
    "y_gt = f(X_gt)\n",
    "\n",
    "plt.scatter(X_train, y_train, color='black')\n",
    "plt.plot(X_gt, y_gt, linewidth=3)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(axis=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb8ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_info_box(ax, degree, score_train, score_test,):\n",
    "    \"\"\"Add a text box with information about the model to the plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    info = r\"Degree: %-3d\" % degree\n",
    "    info += \"\\n\" + r\"$MSE_{train}$: %.2g\" % score_train\n",
    "    info += \"\\n\" + r\"$MSE_{test}$: %.2g\" % score_test\n",
    "    \n",
    "    # Set widht of the text box\n",
    "    axes[i].text(0.55, 1.50, info, \n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='top', \n",
    "                 zorder=10000,\n",
    "                 fontsize=7,\n",
    "                 usetex=False,\n",
    "                 bbox=dict(facecolor='white', \n",
    "                           alpha=0.8,\n",
    "                           edgecolor='black', \n",
    "                           boxstyle='round,pad=0.3'))\n",
    "\n",
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a range of polynomial regression models, with and \n",
    "# without regularization\n",
    "degrees = np.array([2, 3, 4, 10, 50, 100])\n",
    "models_plain = []\n",
    "models_ridge = []\n",
    "models_lasso = []\n",
    "\n",
    "scores_train = []\n",
    "scores_test = []\n",
    "\n",
    "\n",
    "def train_test_model(regressor, degree, X_train, y_train, X_test, y_test):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), regressor)\n",
    "    model.fit(X_train, y_train)\n",
    "    score_train = mean_squared_error(y_train, model.predict(X_train))\n",
    "    score_test = mean_squared_error(y_test, model.predict(X_test))\n",
    "    return model, score_train, score_test\n",
    "\n",
    "# Train and collect the performance of the models.\n",
    "for i, degree in enumerate(degrees):\n",
    "    \n",
    "    info = train_test_model(LinearRegression(), degree, \n",
    "                            X_train, y_train, \n",
    "                            X_test, y_test)\n",
    "    models_plain.append(info)\n",
    "\n",
    "\n",
    "    info = train_test_model(Ridge(alpha=1e-3), \n",
    "                            degree, \n",
    "                            X_train, y_train, \n",
    "                            X_test, y_test)\n",
    "    models_ridge.append(info)\n",
    "    \n",
    "    info = train_test_model(Lasso(alpha=1e-3, max_iter=10000), \n",
    "                            degree, \n",
    "                            X_train, y_train, \n",
    "                            X_test, y_test)\n",
    "    models_lasso.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b0092",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = degrees.size // 3, 3\n",
    "fig, axes = plt.subplots(h, w, figsize=(9, h*2.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "blue,red,green = ml.PALETTE[:3]\n",
    "\n",
    "# Plot the models    \n",
    "for i, degree in enumerate(degrees):\n",
    "    \n",
    "    print(f\"Degree: {degree}\")\n",
    "    print(\"       MSE train    MSE test\")\n",
    "        \n",
    "    \n",
    "    axes[i].scatter(X_train, y_train, color=\"black\", s=10)\n",
    "    #axes[i].scatter(X_test, y_test, marker=\".\", color='blue')\n",
    "    \n",
    "    model, score_train, score_test = models_plain[i]\n",
    "    x = np.linspace(X_train.min(), X_train.max(), 200)\n",
    "    axes[i].plot(x, model.predict(x[:, np.newaxis]), color=red, label=\"Plain\")  \n",
    "    add_info_box(ax=axes[i], degree=degree, \n",
    "                 score_train=score_train, \n",
    "                 score_test=score_test)\n",
    "    print(f\"Plain:   {score_train:7.2g}     {score_test:7.2g}\")\n",
    "    \n",
    "    model, score_train, score_test = models_ridge[i]\n",
    "    axes[i].plot(x, model.predict(x[:, np.newaxis]), color=blue, label=\"Ridge\") \n",
    "    print(f\"Ridge:   {score_train:7.2f}     {score_test:7.2f}\") \n",
    "    \n",
    "    model, score_train, score_test = models_lasso[i]\n",
    "    axes[i].plot(x, model.predict(x[:, np.newaxis]), color=green, label=\"Lasso\")\n",
    "    print(f\"Lasso:   {score_train:7.2f}     {score_test:7.2f}\")\n",
    "    print()\n",
    "    \n",
    "    axes[i].set_xlabel(\"x\")\n",
    "    axes[i].set_ylabel(\"y\")\n",
    "    axes[i].grid(axis=\"x\")\n",
    "    axes[i].set_ylim([-1.75, 2.0]) \n",
    "    \n",
    "\n",
    "# Legend outside the plot\n",
    "axes[2].legend(loc='upper left', bbox_to_anchor=(1.05, 1.05))\n",
    "fig.tight_layout(w_pad=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b819be8a",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- Regularization can help to reduce overfitting by penalizing large coefficients.\n",
    "- The linear regression model has a high variance and low bias, which results in overfitting.\n",
    "- The ridge and lasso regression model have a lower variance and higher bias, which results in underfitting.\n",
    "- The parameter Î± controls the strength of the regularization.\n",
    "- It is a hyperparameter that needs to be tuned. It can be found using cross-validation and grid search.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of a hyperparameter search for Ridge regression:\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# The values of alpha to search\n",
    "alphas = np.logspace(-5, 5, 100)\n",
    "\n",
    "# We use here the Pipeline object to chain together the PolynomialFeatures\n",
    "# and Ridge regression objects. In contrast to make_pipeline(), we can give \n",
    "# the steps names, which is useful when we want to address the parameters of \n",
    "# the steps in the grid search.\n",
    "pipeline = Pipeline([(\"poly\", PolynomialFeatures(degree=30)), \n",
    "                     (\"ridge\", Ridge())])\n",
    "\n",
    "# The parameter grid is a dictionary with the parameter\n",
    "# names as keys and the values to search as values\n",
    "# We can address parameters of the different stages of the pipeline\n",
    "# using the double underscore \"__\" separator: \"ridge__alpha\"\n",
    "param_grid = {\"ridge__alpha\": alphas}\n",
    "\n",
    "# Run the grid search with 5-fold cross-validation.\n",
    "# Use the MSE as the scoring metric, or \"neg_mean_squared_error\"\n",
    "# because GridSearchCV always maximizes the score..!\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid = GridSearchCV(pipeline,\n",
    "                    param_grid, cv=kf, return_train_score=True,\n",
    "                    scoring=\"neg_mean_squared_error\")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Let's print the best alpha and the best score\n",
    "print(f\"Best alpha: {grid.best_params_['ridge__alpha']:.2g}\")\n",
    "print(f\"Best score: {grid.best_score_:.2g}\")\n",
    "\n",
    "# Plot the results of the cross-validation\n",
    "plt.figure()\n",
    "plt.semilogx(alphas, grid.cv_results_[\"mean_test_score\"], \"-o\", label=\"Test score\")\n",
    "plt.semilogx(alphas, grid.cv_results_[\"mean_train_score\"], \"-o\", label=\"Train score\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid(axis=\"x\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ml-hs24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
