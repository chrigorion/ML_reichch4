{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067ed245",
   "metadata": {},
   "source": [
    "# **SW03: Training, Validation, Testing**\n",
    "\n",
    "In this Jupyter notebook, we look at the different possibilities how to \n",
    "set up a machine learning pipeline. First we learn how to split the data\n",
    "into training and testing sets. Then we look at three different ways to\n",
    "set up a machine learning pipeline:\n",
    "\n",
    "- Discouraged: No splitting\n",
    "- Okay practice: Train-test split\n",
    "- Good practice: Train-validation-test split\n",
    "- Recommended practice: Training with cross-validation\n",
    "- Rolls-royce practice: Training with nested cross-validation\n",
    "\n",
    "In the following, we will work once more with the [iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html), which you already have seen.\n",
    "\n",
    "Before we create a machine learning pipeline, let's first see how we can split the initial dataset into subsets for training and testing.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2485c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Some Jupyter magic for nicer output\n",
    "%config InlineBackend.figure_formats = [\"svg\"]   # Enable vectorized graphics\n",
    "\n",
    "# Jupyter / IPython configuration:\n",
    "# Automatically reload modules when modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import ml\n",
    "\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d9e8fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_iris(as_frame=True, \n",
    "                            return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe3d816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                  5.1               3.5                1.4               0.2\n",
      "1                  4.9               3.0                1.4               0.2\n",
      "2                  4.7               3.2                1.3               0.2\n",
      "3                  4.6               3.1                1.5               0.2\n",
      "4                  5.0               3.6                1.4               0.2\n",
      "..                 ...               ...                ...               ...\n",
      "145                6.7               3.0                5.2               2.3\n",
      "146                6.3               2.5                5.0               1.9\n",
      "147                6.5               3.0                5.2               2.0\n",
      "148                6.2               3.4                5.4               2.3\n",
      "149                5.9               3.0                5.1               1.8\n",
      "\n",
      "[150 rows x 4 columns] 0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "145    2\n",
      "146    2\n",
      "147    2\n",
      "148    2\n",
      "149    2\n",
      "Name: target, Length: 150, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df27e04",
   "metadata": {},
   "source": [
    "## **Creating data splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22389861",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 1    ###\n",
    "########################\n",
    "\n",
    "# Split the data into training and test sets using your own implementation.\n",
    "# The training set should contain 80% of the data, and the test set 20%.\n",
    "# You are allowed to use numpy functions for this task.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad38dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 2    ###\n",
    "########################\n",
    "\n",
    "# What is the problem with the below code?\n",
    "\n",
    "def bad_shuffler_numpy(X, y):        \n",
    "    n = len(X)\n",
    "    split = int(0.8 * n)\n",
    "    np.random.shuffle(X)\n",
    "    np.random.shuffle(y)\n",
    "    X_train_WRONG, X_test_WRONG = X[:split], X[split:]\n",
    "    y_train_WRONG, y_test_WRONG = y[:split], y[split:]\n",
    "    return X_train_WRONG, X_test_WRONG, y_train_WRONG, y_test_WRONG\n",
    "    \n",
    "X_np = X.to_numpy(copy=True)\n",
    "y_np = y.to_numpy(copy=True)\n",
    "ret = bad_shuffler_numpy(X=X_np, y=y_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a8c575",
   "metadata": {},
   "source": [
    "Everything is simpler with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# See how the size of the training and test sets compare:\n",
    "print(f\"Training set size: {len(X_train):>3}\")\n",
    "print(f\"Test set size:     {len(X_test):>3}\")\n",
    "print(f\"Total size:        {len(X):>3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 3    ###\n",
    "########################\n",
    "\n",
    "#Â a) How to split a dataset into training and test sets such that the the ratio \n",
    "#    training and test set 3:1?\n",
    "\n",
    "# b) We use the iris dataset which has 3 target classes (the different types \n",
    "#    of iris flowers). The complete dataset has 150, with 50 samples per class.\n",
    "#    Will the ratio of the classes be preserved in the training and test sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361948f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we get different shuffles each time we run the code\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "display(X_train.head())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "display(X_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e447d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducing the same results, we can set the random_state parameter\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "display(X_train.head())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "display(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6863625",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Splitting data for model development**\n",
    "\n",
    "In the following, we are going to see different procedures to split the data\n",
    "into training and testing sets. The most basic procedure is the train-test\n",
    "split, where a fraction of the data is used for training and the rest for\n",
    "testing. The use of a validation set is also common, where the data is split\n",
    "into three sets: training, validation, and test sets. The validation set is\n",
    "used to tune the hyperparameters of the model. Cross-validation is another \n",
    "common procedure, where the data is split into *multiple* training and test \n",
    "sets. This is useful to get a more robust estimate of the model's performance\n",
    "on unseen data. Before introducing these procedures, we start with the \n",
    "discouraged practice of not splitting the data at all.\n",
    "\n",
    "Note that the data is usually shuffled before splitting to ensure that the\n",
    "training and test sets are representative of the data. When using random \n",
    "shuffling, it is a good practice to set the random_state parameter to ensure\n",
    "reproducibility of the results.\n",
    "\n",
    "We continue to work with the iris dataset. As it is a multi-class classification\n",
    "problem, we are going to use the accuracy score to evaluate the model's\n",
    "performance. The accuracy score is the fraction of correctly predicted samples\n",
    "out of the total number of samples. It is defined as:\n",
    "\n",
    "$$\\text{accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "where TP is the number of true positives, TN the number of true negatives, FP\n",
    "the number of false positives, and FN the number of false negatives. We will\n",
    "learn more about these concepts soon. Here, we will simply use the\n",
    "[`accuracy_score(y_true, y_pred)`](https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.accuracy_score.html) \n",
    "function from scikit-learn to calculate the accuracy of the model, given the true \n",
    "target labels `y_true` and the predicted labels `y_pred`.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### **Method 1: No splitting (BAD)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Train a polynomial logistic regression model on the training set\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a pipeline that first transforms the data to include polynomial\n",
    "# features, and then fits a logistic regression model to the data\n",
    "model = make_pipeline(PolynomialFeatures(degree=2), \n",
    "                      LogisticRegression(max_iter=1000))\n",
    "model.fit(X, y)\n",
    "\n",
    "#Â Evaluate the model\n",
    "y_pred = model.predict(X)\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2191ed",
   "metadata": {},
   "source": [
    "Not to split the data is the simplest, but also the wrongest way to train and \n",
    "evaluate a model!\n",
    "\n",
    "**The problem**: We have no way to evaluate the model on unseen \n",
    "data. How will it perform on new data? It seems to work well on the data we \n",
    "have used for training, but we might just have overfit the model to the \n",
    "training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "660bce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 4    ###\n",
    "########################\n",
    "\n",
    "# Imagine we shuffle the data before training the model. Will the model\n",
    "# performance be affected?\n",
    "idx = np.arange(len(X))\n",
    "np.random.seed(42)  # Set seed for reproducibility\n",
    "np.random.shuffle(idx)\n",
    "X, y = X.iloc[idx], y.iloc[idx]\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc997d7",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### **Method 2: Train-test split**\n",
    "\n",
    "This is the most basic way to split the model development process into\n",
    "training and testing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f3ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=41)\n",
    "\n",
    "# Same type of model...\n",
    "model = make_pipeline(PolynomialFeatures(degree=2), \n",
    "                      LogisticRegression(max_iter=1000))\n",
    "\n",
    "# ...but this time we train the model on the training set.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Â Now, we can evaluate the model on the test set!\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# If we want, we can compare the accuracy on the training set as well\n",
    "y_pred = model.predict(X_train)\n",
    "accuracy_train = accuracy_score(y_train, y_pred)\n",
    "\n",
    "print(f\"Accuracy on training set (in-sample): {accuracy_train:.2f}\")\n",
    "print(f\"Accuracy on test set (out-of-sample): {accuracy_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c96268",
   "metadata": {},
   "source": [
    "This is a much better way to assess the perforamnce of the model, as it\n",
    "gives us an idea of how well the model **generalizes** to **unseen data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78b5b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 5    ###\n",
    "########################\n",
    "\n",
    "# The accuracy on the training set is usually higher than the accuracy on the\n",
    "# test set. Can you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "673c1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 6    ###\n",
    "########################\n",
    "\n",
    "# In the previous exercise, we noted that the accuracy on the training set is\n",
    "# usually higher than the accuracy on the test set. However, in some cases, the\n",
    "# accuracy on the test set can be higher than the accuracy on the training set.\n",
    "# Can you think of a reason why this might happen? \n",
    "#\n",
    "# Hint: You can verify this with the above code by using a different seed point\n",
    "# for the random number generator (random_state = ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0f180",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "### **Method 3: Train-test-validation split**\n",
    "\n",
    "In the above example, we used a polynomial logistic regression model to\n",
    "classify the iris dataset. In this model, we can tune the degree of the\n",
    "polynomial features to improve the performance of the model. This tuning\n",
    "parameter is not trained when we fit the model to the data, however it \n",
    "affects the performance of the model. Such a parameter is called a\n",
    "**hyperparameter**.\n",
    "\n",
    "In order to tune the hyperparameters of the model, we can use a **train-\n",
    "validation-test split**. The training set is used to train the model, the\n",
    "validation set is used to tune the hyperparameters of the model, and the\n",
    "test set is used to evaluate the performance of the model.\n",
    "\n",
    "Here is how we can create such a split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, test sets (ratio: 4:1)\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, \n",
    "                                                          test_size=0.2, \n",
    "                                                          random_state=42)\n",
    "\n",
    "# Split the training set further into training and validation sets (ratio: 3:1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, \n",
    "                                                  test_size=0.25, \n",
    "                                                  random_state=42)\n",
    "\n",
    "# Train the model on the training set and optimize the hyperparameters (here:\n",
    "# the degree of the polynomial features) on the validation set.\n",
    "\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "accuracies = []\n",
    "\n",
    "for degree in degrees:\n",
    "    model = make_pipeline(PolynomialFeatures(degree=degree), \n",
    "                          LogisticRegression(max_iter=10000))\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "print(\"Validation accuracies:\")\n",
    "for degree, accuracy in zip(degrees, accuracies):\n",
    "    print(f\" - Degree {degree}: {accuracy:.3f}\")\n",
    "\n",
    "# Find the best degree\n",
    "best_degree = degrees[np.argmax(accuracies)]\n",
    "print(f\"\\nBest degree: {best_degree}\")\n",
    "\n",
    "# Train the model on the training set (train+val) using the best degree.\n",
    "model = make_pipeline(PolynomialFeatures(degree=best_degree), \n",
    "                      LogisticRegression(max_iter=1000))\n",
    "model.fit(X_trainval, y_trainval)\n",
    "\n",
    "# Evaluate the model on the test set.\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy on test set: {accuracy_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09d55aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 7    ###\n",
    "########################\n",
    "\n",
    "# Why is it a good idea to split the data into training, validation, and test\n",
    "# sets when optimizing the hyperparameters of a model? Why should we not use\n",
    "# the test set for hyperparameter optimization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dddacb",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "### **Method 4: Cross-validation**\n",
    "\n",
    "Cross-validation is a model evaluation technique that is used to estimate the\n",
    "performance of a model on unseen data. It can be seen as a generalization of\n",
    "the train-test split, where the data is split into multiple training and test\n",
    "sets. Essentially, we split the data *k* different train-test splits, and\n",
    "train and test the model on each of these splits. The performance of the model\n",
    "is then averaged over all the splits to get an estimate of the performance of\n",
    "the model on unseen data. Because we average the performance over multiple\n",
    "splits, cross-validation provides a more reliable estimate of the performance\n",
    "of the model than a single train-test split.\n",
    "\n",
    "The term \"cross-validation\" is a bit misleading, as we use the technique to\n",
    "evaluate (= *test*) the model, and to estimate its generalization error. The \n",
    "returned score values are computed on the *test* sets (not the training sets).\n",
    "\n",
    "The concept of cross-validation is explained in more details in the following\n",
    "sources:\n",
    "- https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "- https://machinelearningmastery.com/k-fold-cross-validation/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b68483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "from sklearn.model_selection import (cross_val_score, \n",
    "                                     cross_validate)\n",
    "\n",
    "# Create a pipeline that first transforms the data to include polynomial\n",
    "# features, and then fits a logistic regression model to the data\n",
    "model = make_pipeline(PolynomialFeatures(degree=2), \n",
    "                      LogisticRegression(max_iter=1000))\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring=\"accuracy\")\n",
    "print(f\"Cross-validation scores:     {scores.round(2)}\")\n",
    "print(f\"Mean cross-validation score:  {scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f02727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can return more information from the cross-validation using the\n",
    "# cross_validate() function instead of cross_val_score().\n",
    "results = cross_validate(model, X, y, cv=5, scoring=\"accuracy\",\n",
    "                         return_train_score=True,\n",
    "                         return_estimator=True)\n",
    "print(\"Available info: \", results.keys())\n",
    "print(\"Train scores:   Mean: %.2f %s\" % (results[\"train_score\"].mean(), \n",
    "                                         results[\"train_score\"].round(2)))\n",
    "print(\"Test scores:    Mean: %.2f %s\" % (results[\"test_score\"].mean(), \n",
    "                                         results[\"test_score\"].round(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eda580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, by default, there is no shuffling of the data in cross-validation!\n",
    "# To shuffle the data, and also to control the random seed, we can use the\n",
    "# KFold object from scikit-learn. This is the instance that controls the \n",
    "# splitting of the data.\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = cross_validate(model, X, y, cv=cv, scoring=\"accuracy\",\n",
    "                         return_train_score=True,\n",
    "                         return_estimator=True)\n",
    "print(\"Available info: \", results.keys())\n",
    "print(\"Train scores:   Mean: %.2f %s\" % (results[\"train_score\"].mean(), \n",
    "                                         results[\"train_score\"].round(2)))\n",
    "print(\"Test scores:    Mean: %.2f %s\" % (results[\"test_score\"].mean(), \n",
    "                                         results[\"test_score\"].round(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "524c2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 8    ###\n",
    "########################\n",
    "\n",
    "# a) What is the difference between cross-validation and a train-test split?\n",
    "# b) Why is cross-validation a better estimate of the model performance than a\n",
    "#    train-test split?\n",
    "# c) How many times will each sample be used for training in a 5-fold \n",
    "#    cross-validation? And how many times it will be used for testing?\n",
    "# d) What is the difference between the train_score and test_score in the\n",
    "#    cross_validate function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6519ec6d",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "### **Method 5: Nested cross-validation**\n",
    "\n",
    "We have seen how to generalize the train-test split to cross-validation.\n",
    "We can also generalize the train-validation-test split to cross-validation.\n",
    "This will lead to a so-called nested cross-validation scheme. Here, we will\n",
    "use the inner cross-validation loop to optimize the hyperparameters of the\n",
    "model, and the outer cross-validation loop to estimate the performance of the\n",
    "model on new, unseen data.\n",
    "\n",
    "We will revisit nested cross-validation when we discuss hyperparameter tuning\n",
    "and model selection in more detail.\n",
    "\n",
    "Further reading:\n",
    "- Machine learning mastery: [Nested cross-validation for machine learning](https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/)\n",
    "- scikit-learn: [Nested vs. non-nested cross-validation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed45b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Inner cross-validation: Used to optimize the hyperparameters of the model\n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "# Outer cross-validation: Used to estimate the performance of the model\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=43)\n",
    "# The model pipeline. Note that we do not specify the degree of the polynomial\n",
    "# features here, as we will optimize this using grid search.\n",
    "model = make_pipeline(PolynomialFeatures(), \n",
    "                      LogisticRegression(max_iter=10000))\n",
    "\n",
    "# Parameters for grid search (degree of polynomial features).\n",
    "# Because we are using a pipeline, we need to specify the parameter name\n",
    "# using the name of the step in the pipeline, followed by two underscores.\n",
    "p_grid = {\"polynomialfeatures__degree\": [1, 2, 3, 4, 5]}\n",
    "\n",
    "# Perform grid search (inner cv) with k-fold cross-validation (outer cv)\n",
    "classifier = GridSearchCV(estimator=model, \n",
    "                          param_grid=p_grid, \n",
    "                          cv=inner_cv, \n",
    "                          scoring=\"accuracy\")\n",
    "results = cross_validate(classifier, X=X, y=y, cv=outer_cv,\n",
    "                         return_train_score=True)\n",
    "\n",
    "print(\"Train scores:   Mean: %.2f %s\" % (results[\"train_score\"].mean(), \n",
    "                                         results[\"train_score\"].round(2)))\n",
    "print(\"Test scores:    Mean: %.2f %s\" % (results[\"test_score\"].mean(), \n",
    "                                         results[\"test_score\"].round(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c456bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 9    ###\n",
    "########################\n",
    "\n",
    "# a) How many times will the model be trained and evaluated in the above code?\n",
    "# b) How many times will each sample be used for training in the above nested\n",
    "#    cross-validation setup? How many times will it be used for validation?\n",
    "#    How many times will it be used for testing?\n",
    "# c) What is the difference between the inner and outer cross-validation in\n",
    "#    the above code?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa824911",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "### **Other methods**\n",
    "\n",
    "We have seen basic ways to split the data for cross-validation using k-fold \n",
    "splitting. There are many other schemes for splitting the data, which we \n",
    "will not cover in detail here:\n",
    "- Leave-one-out \n",
    "- Leave-p-out \n",
    "- Shuffle-split\n",
    "- Stratified split\n",
    "- Group split\n",
    "\n",
    "Further reading:\n",
    "- scikit-learn: [Cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
