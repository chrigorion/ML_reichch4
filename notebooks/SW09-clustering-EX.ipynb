{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e1c31c4-faf7-4f80-a6f1-6a22a410570f",
   "metadata": {},
   "source": [
    "# **SW09: Clustering**\n",
    "\n",
    "This notebook demonstrates clustering using k-means and DBSCAN. Furthermore, we explore the elbow method to find the optimal number of clusters (for k-means) and evaluate the clustering results using the silhouette score.\n",
    "\n",
    "In addition to this notebook, you should take a look at the following interesting [web resource on clustering](https://educlust.dbvis.de/#)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec53ed0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Setup**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Some Jupyter magic for nicer output\n",
    "%config InlineBackend.figure_formats = [\"svg\"]   # Enable vectorized graphics\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Adjust the default settings for plots\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import ml\n",
    "ml.setup_plotting()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff3d8c3",
   "metadata": {},
   "source": [
    "## **Data preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The make_blobs function generates random data points around a specified \n",
    "# number of centers. We will use this function to generate a dataset with\n",
    "# 500 data points and 4 centers.\n",
    "X, _ = make_blobs(n_samples=500, centers=4, cluster_std=1.7, random_state=42)\n",
    "data = pd.DataFrame(X, columns=[\"Feature 1\", \"Feature 2\"])\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(data[\"Feature 1\"], data[\"Feature 2\"], \n",
    "            color=\"gray\", s=10, alpha=0.6, edgecolor=\"gray\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Synthetic data\", fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084d112",
   "metadata": {},
   "source": [
    "### **Data normalization**\n",
    "\n",
    "Normalization ensures that all features contribute equally to the clustering \n",
    "process by scaling them to a similar range. This is crucial in distance-based \n",
    "algorithms like k-means, where differences in feature scales can distort the \n",
    "clustering results. Here, we will use the \n",
    "[StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) \n",
    "from scikit-learn to normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221cae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40faa57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **k-means clustering**\n",
    "\n",
    "k-means clustering is a popular unsupervised machine learning algorithm used to partition a dataset into distinct groups or clusters. The algorithm aims to minimize the variance within each cluster and maximize the variance between different clusters. It achieves this by iteratively assigning data points to clusters based on their proximity to the cluster centroids and then updating the centroids to be the mean of the data points assigned to them.\n",
    "\n",
    "Running k-means clustering on the data is generally straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9602551",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(data[\"Feature 1\"], data[\"Feature 2\"], \n",
    "            c=labels, s=10, cmap=\"viridis\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"k-means clustering\", fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a869a0",
   "metadata": {},
   "source": [
    "### **Inertia**\n",
    "\n",
    "The intertia (also known as the *within-cluster sum of squares* (WCSS)) measures\n",
    "the sum of squared distances of samples to their closest cluster center. \n",
    "If $\\mu_j$ is the center of cluster $C_j$, and $\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)$\n",
    "is the distance of the data point $x_i$ to its closest cluster center, then the inertia is given by:\n",
    "\n",
    "$$\\sum_{i=0}^{n} \\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)$$\n",
    "\n",
    "The inertia is a measure of how well the data points are clustered.\n",
    "The lower, the better the clustering. scikit-learn provides the inertia as \n",
    "an attribute of the KMeans object: `kmeans.inertia_`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 1    ###\n",
    "########################\n",
    "\n",
    "# Compute the inertia for the k-means model using only Python/NumPy only.\n",
    "# \n",
    "# Hint: The kmeans.labels_ attribute contains the cluster assignments for each\n",
    "#       data point. The kmeans.cluster_centers_ attribute contains the cluster\n",
    "#       centers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc46933",
   "metadata": {},
   "source": [
    "### **The elbow method**\n",
    "\n",
    "The parameter $k$ is not known in advance, so we need to find a way to determine\n",
    "the optimal number of clusters. One (heuristic) way to do this is to use the elbow method.\n",
    "It consists of plotting the inertia as a function of the number\n",
    "of clusters and looking for the \"elbow\" point, where the inertia starts to\n",
    "decrease more slowly. This point is usually a good choice for the number of\n",
    "clusters. Let's verify this for our synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 2    ###\n",
    "########################\n",
    "\n",
    "# Compute k-means clustering for k=2 to k=10 and plot the inertia as a function\n",
    "# of the number of clusters. What can you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7989e14",
   "metadata": {},
   "source": [
    "### **Silhouette score**\n",
    "\n",
    "While the inertia (sum of squared distances to the nearest cluster center) is a\n",
    "useful metric for evaluating the quality of a clustering, it has some shortcomings:\n",
    "- It is not normalized and depends on the number of data points. It is therefore not directly\n",
    "  comparable across datasets.\n",
    "- Noise and outliers can have a large impact on the inertia, which can be problematic\n",
    "- It assumes that clusters are spherical and equally sized, which is not always the case. \n",
    "\n",
    "\n",
    "The silhouette score is a metric that addresses some of these shortcomings. It measures\n",
    "how similar a sample $i$ is to its own cluster compared to other clusters. It is defined as:\n",
    "$$ s_i = \\frac{b_i - a_i}{\\max(a_i, b_i)} $$\n",
    "Â where $a_i$ is the average distance of sample $i$ to all other points in the same cluster, and\n",
    "$b_i$ is the average distance of sample $i$ to all points in the nearest cluster. The total silhouette\n",
    "score is then the average of $s_i$ over all samples.\n",
    "\n",
    "The score ranges from -1 to 1, where a high value indicates that the sample is well matched to\n",
    "its own cluster and poorly matched to neighboring clusters. A negative value indicates that\n",
    "the sample might be assigned to the wrong cluster.\n",
    "\n",
    "The silhouette score can be easily computed using the [`silhouette_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) function from sklearn.metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d617dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 3    ###\n",
    "########################\n",
    "\n",
    "# Compute the silhouette score for k=2 to k=10 and plot the silhouette score as a function\n",
    "# of the number of clusters. What can you observe?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e247d53",
   "metadata": {},
   "source": [
    "## **DBSCAN**\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that groups points based on their density, identifying clusters of arbitrary shapes and labeling low-density points as noise. It does not require pre-specifying the number of clusters and is robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256c8c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.3, min_samples=3)\n",
    "labels = dbscan.fit_predict(X)\n",
    "dbscan_silhouette = silhouette_score(X[labels != -1], labels[labels != -1])\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap=\"viridis\", alpha=0.6)\n",
    "plt.title(f\"DBSCAN Clustering (Silhouette={dbscan_silhouette:.2f})\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761881e2",
   "metadata": {},
   "source": [
    "**Discussion**:\n",
    "- DBSCAN is not able to detect the correct number of clusters, even though the\n",
    "  silhouette score is quite high. This is because DBSCAN is not able to detect\n",
    "  the correct number of clusters in this dataset.\n",
    "- DBSCAN is more suitable for datasets with varying cluster densities and shpaes, where\n",
    "  the clusters are not necessarily spherical.\n",
    "- In this case, k-means is the better choice, as the clusters are spherical and well-separated.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ml-hs24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
