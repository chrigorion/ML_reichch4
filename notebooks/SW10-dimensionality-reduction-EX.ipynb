{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SW10: Dimensionality reduction**\n",
    "\n",
    "In this tutorial, we apply the following dimensionality reduction techniques:\n",
    "\n",
    "* Feature selection\n",
    "* Linear disciminant analysis\n",
    "* Principal component analysis\n",
    "* t-SNE (for visualization)\n",
    "\n",
    "The focus of this exercise is that you can apply the methods to some data.\n",
    "\n",
    "Note: Every exercise in this notebook starts with the original data provided, \n",
    "In practice, one would combine the methods, begin with some feature selection,\n",
    "before proceeding to further dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Setup**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Some Jupyter magic for nicer output\n",
    "%config InlineBackend.figure_formats = [\"svg\"]   # Enable vectorized graphics\n",
    "\n",
    "# Automatically reload modules that have changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Adjust the default settings for plots\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import ml\n",
    "ml.setup_plotting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Data preparation**\n",
    "\n",
    "The [make_blobs()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) \n",
    "function from scikit-learn generates random data points for a specified number \n",
    "of clusters. The data points are normally distributed around the cluster \n",
    "centers with a specified standard deviation. We then further stretch the data \n",
    "points to create elongated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5000     # Number of samples to generate\n",
    "seed = 42            # Set to None to generate different data each time\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=n_samples, \n",
    "                           n_classes=3,\n",
    "                           n_features=11, \n",
    "                           n_informative=3, \n",
    "                           n_redundant=3, \n",
    "                           n_repeated=2,\n",
    "                           n_clusters_per_class=1, \n",
    "                           class_sep=2.0,\n",
    "                           random_state=seed)\n",
    "\n",
    "# Convert y into a categorical variable (with text labels)\n",
    "y = pd.Categorical(y, categories=[0, 1, 2], ordered=True)\n",
    "y = y.rename_categories([\"Class 0\", \"Class 1\", \"Class 2\"])\n",
    "\n",
    "print(\"Number of samples:  %4d\" % X.shape[0])\n",
    "print(\"Number of features: %4d\" % X.shape[1])\n",
    "\n",
    "df = pd.DataFrame(X, columns=[f\"Feature {i}\" for i in range(X.shape[1])])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Apply feature selection**\n",
    "\n",
    "In the lecture, we have learned multiple ways to select the best features for a model.\n",
    "In this Jupyter notebook, we limit ourselves to two methods: \n",
    "- Identify redundant features using the correlation matrix\n",
    "- Use the feature importance from a tree-based model\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Remove redundant features**\n",
    "\n",
    "Features are redundant if they are linear combinations of each other or \n",
    "if they contain the same information in some other way. In a first step, \n",
    "let's try to identify redundant features by calculating the correlation\n",
    "matrix of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 1    ###\n",
    "########################\n",
    "\n",
    "# Given the feature matrix X, identify the features that are redundant.\n",
    "# \n",
    "# Instructions:\n",
    "#   1. Recapitulate the meaning of the correlation matrix\n",
    "#   2. Calculate the correlation matrix of X\n",
    "#   3. Visualize the correlation matrix using a heatmap\n",
    "#   4. Identify the redundant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Select by feature importance**\n",
    "\n",
    "In a previous lecture, we have seen that feature importance is a useful tool\n",
    "to identify the most relevant features for a given problem. Let's try this \n",
    "together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 2    ###\n",
    "########################\n",
    "\n",
    "# Given the feature matrix X, identify the features that are irrelevant.\n",
    "#\n",
    "# Instructions:\n",
    "#   1. Recapitulate the meaning feature importance\n",
    "#   2. Calculate the feature importance of each feature\n",
    "#   3. Visualize the feature importance using a bar plot\n",
    "#   4. Identify the irrelevant features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Apply LDA**\n",
    "\n",
    "The linear discriminant analysis (LDA) is a method that reduces the \n",
    "dimensionality of the data by finding a low-dimensional space that maximizes\n",
    "the separation between classes. For a binary classification problem, LDA\n",
    "finds a 1D-direction in the feature space that maximizes the separation\n",
    "between the two classes. For a multi-class classification problem with C \n",
    "classes, LDA finds a (C-1)-dimensional space that maximizes the separation.\n",
    "\n",
    "Have a look at the extra notebook on LDA for more information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 3    ###\n",
    "########################\n",
    "\n",
    "# Given the feature matrix X and the target vector y, compute the \n",
    "# Linear Discriminant Analysis (LDA) projection of the data and \n",
    "# visualize the first two components.\n",
    "\n",
    "# Instructions:\n",
    "#   1. Recapitulate the meaning of LDA (see notebook on LDA!)\n",
    "#   2. Fit an LDA model to the data\n",
    "#   3. Transform the data using the LDA model\n",
    "#   4. Visualize the first two components\n",
    "\n",
    "# Note: We look here at a multi-class classification problem, with 3 classes.\n",
    "#       For 3 classes, LDA will be able to project the data into a \n",
    "#       2-dimensional space..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Principal component analysis**\n",
    "\n",
    "The principal component analysis (PCA) is a method for dimensionality reduction.\n",
    "Contrary to LDA, it is an unsupervised method, which means that it does not use\n",
    "the target variable for the dimensionality reduction. Instead, PCA aims to find\n",
    "the directions in the data that explain the most variance. The directions are\n",
    "called principal components, and the method projects the data onto these components.\n",
    "The principal components are orthogonal to each other, which means that they are\n",
    "uncorrelated. The principal components are ordered by the amount of variance they\n",
    "explain in the data. The first principal component explains the most variance, the\n",
    "second principal component explains the second most variance, and so on. \n",
    "The PCA projection is useful for visualization of the data, and for reducing the\n",
    "dimensionality of the data. The PCA projection can be used as input for other\n",
    "machine learning algorithms, such as clustering, classification, or regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 4    ###\n",
    "########################\n",
    "\n",
    "# Given the feature matrix X, compute the Principal Component Analysis (PCA)\n",
    "# projection of the data and visualize the first two components.\n",
    "\n",
    "# Instructions:\n",
    "#   1. Recapitulate the meaning of PCA (see notebook on PCA!)\n",
    "#   2. Fit a PCA model to the data\n",
    "#   3. Transform the data using the PCA model\n",
    "#   4. Visualize the first two components\n",
    "#   5. Compare the results of LDA and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    SOLUTION 4    ###\n",
    "########################\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Recap PCA:\n",
    "################\n",
    "#    - Principal Component Analysis (PCA) is a method for dimensionality\n",
    "#      reduction and data visualization.\n",
    "#    - PCA aims to find the linear combinations of features that capture the\n",
    "#      most variance in the data.\n",
    "#    - The method finds the directions in the data that capture the most\n",
    "#      variance, called the principal components.\n",
    "#    - The principal components are ordered by the amount of variance they\n",
    "#      capture. The first principal component captures the most variance, the\n",
    "#      second principal component captures the second most variance, and so on.\n",
    "\n",
    "# 2. Fit a PCA model to the data\n",
    "#################################\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "# 3. Transform the data using the PCA model\n",
    "############################################\n",
    "X_reduced = pca.transform(X)\n",
    "\n",
    "# 4. Visualize the first two components\n",
    "########################################\n",
    "sns.scatterplot(x=X_reduced[:,0], y=X_reduced[:,1], hue=pd.Categorical(y), \n",
    "                alpha=0.5, palette=ml.PALETTE_CMAP.colors)\n",
    "plt.show()\n",
    "\n",
    "# 5. Compare the results of LDA and PCA\n",
    "########################################\n",
    "# The PCA projection shows the directions in the data that capture the most\n",
    "# variance. The LDA projection shows the directions in the data that best\n",
    "# separate the classes. In this case, the LDA projection shows a clear\n",
    "# separation between the classes, while the PCA projection shows the directions\n",
    "# in the data that capture the most variance, but does not separate the classes\n",
    "# as well as LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 5    ###\n",
    "########################\n",
    "\n",
    "# For visualization, we limit the dimensions to 2. However, in practice, we\n",
    "# may want to use more dimensions. Compute the explained variance ratio for\n",
    "# all principal components and visualize the cumulative explained variance.\n",
    "# Which is the best choise for the number of components to include?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Non-linear dimensionality reduction**\n",
    "\n",
    "t-SNE is a non-linear dimensionality reduction technique that is often used\n",
    "for visualization of high-dimensional data. The method aims to find a\n",
    "low-dimensional representation of the data that preserves the local\n",
    "structure of the data points. \n",
    "\n",
    "Because t-SNE does not preserve distances, it is not suitable for several\n",
    "applications, such as distance-based clustering or classification. However,\n",
    "it is very useful for visualization of high-dimensional data, where it can\n",
    "reveal patterns and clusters that are not visible in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Fit a t-SNE model to the data\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Transform the data using the t-SNE model\n",
    "# This step is relatively slow!\n",
    "X_reduced = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first two components\n",
    "sns.scatterplot(x=X_reduced[:,0], y=X_reduced[:,1], hue=y, \n",
    "                alpha=0.5, palette=ml.PALETTE_CMAP.colors)\n",
    "plt.xlabel(\"t-SNE component 1\")\n",
    "plt.ylabel(\"t-SNE component 2\")\n",
    "plt.title(\"t-SNE projection\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ml-hs24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
