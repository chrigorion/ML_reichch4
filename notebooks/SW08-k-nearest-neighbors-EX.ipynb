{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e1c31c4-faf7-4f80-a6f1-6a22a410570f",
   "metadata": {},
   "source": [
    "# **SW08: k-Nearest Neighbors (kNN)**\n",
    "\n",
    "The k-nearest neighbors (kNN) method is a simple, instance-based learning \n",
    "algorithm used for classification and regression. In kNN, predictions are \n",
    "made based on the *k* closest training examples (neighbors) to a given \n",
    "data point, where *k* is a predefined integer. For classification, the model \n",
    "assigns the most common class among the *k* neighbors (majority voting), while\n",
    "for regression, it averages the values of the neighbors. \n",
    "\n",
    "![Illustration of kNN for classfication and regression](../data/images/knn-illustration.svg?2)\n",
    "\n",
    "In the above illustrations, we see how kNN can be used for classification and regression.\n",
    "On the left, the situation of a classification problem with two feature variables x1 and x2 and \n",
    "two classes (A: red stars, B: green triangles) is shown.\n",
    "For a new data point (blue square), we can predict the class by looking at the *k* nearest neighbors.\n",
    "If *k*=3, the prediction would be class B (2 green triangles vs. 1 red star). If *k*=7, the prediction \n",
    "would be class A (4 red stars vs. 3 green triangles).\n",
    "\n",
    "On the right, the situation of a regression with one feature (*x*-axis) and one target variable (*y*-axis) is depicted.\n",
    "Again, we can predict the value of the target variable (*y*) for a new data point (blue square) by looking at the *k* \n",
    "nearest neighbors in the feature space (here: the x-axis). The prediction is the average of the target values of the\n",
    "*k* nearest neighbors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d097c48",
   "metadata": {},
   "source": [
    "In this notebook, we apply the kNN to solve machine learning tasks. We will be using the iris dataset, which you are already very familiar with. Recall that the iris dataset has **4 features** (sepal length, sepal width, petal\n",
    "length, and petal width) and **3 classes** (setosa, versicolor, and virginica)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec53ed0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Setup**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Some Jupyter magic for nicer output\n",
    "%config InlineBackend.figure_formats = [\"svg\"]   # Enable vectorized graphics\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Adjust the default settings for plots\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import ml\n",
    "ml.setup_plotting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a248ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "data = load_iris(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "display(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40faa57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Basic example**\n",
    "\n",
    "Let's apply kNN directly to predict how it performs on the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da03db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "accuracy_score(y, y_pred)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd129d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 1    ###\n",
    "########################\n",
    "\n",
    "# Part 1:\n",
    "# -------\n",
    "# Play around with the n_neighbors parameter and see how it affects the \n",
    "# accuracy of the model. What is the best value for n_neighbors?\n",
    "\n",
    "# Part 2:\n",
    "# -------\n",
    "# Remember: It is recommended to assess the model's performance on a separate\n",
    "# test set, rather than the training set. Can you update the code such that it \n",
    "# uses a train-test split?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb3ba46-f82d-40cc-a9ca-4539dc6e59da",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Similarity and distance measures**\n",
    "\n",
    "In kNN, the intuition is: The smaller the distance between samples in the \n",
    "feature space, the more similar they are. By identfiying for a new sample \n",
    "the k nearest neighbors in the training set, we find its k most similar\n",
    "samples, which we can use to predict the target of the new sample. Therefore, \n",
    "distance measures are fundamental for kNN, as they determine which \n",
    "training instances are nearest (\"most similar\") to a new data point.\n",
    "\n",
    "The most common distance measure is the **Euclidean distance**, which is defined as:\n",
    "$$d(x_A, x_B) = \\sqrt{\\sum_{i=1}^d (x_{A,i} - x_{B,i})^2}$$\n",
    "It measures the straight-line distance between two points $x_A$ and $x_B$ in \n",
    "feature space. Remember: $x_A$, $x_B$ are numerical vectors of the features. The parameter\n",
    "$d$ refers to the number of features in the dataset. The **Minkowski distance** is a \n",
    "generalization of the Euclidean distance and is defined as:\n",
    "$$d(x_A, x_B) = \\left(\\sum_{i=1}^d |x_{A,i} - x_{B,i}|^p\\right)^{1/p}$$\n",
    "The parameter p determines the type of distance:\n",
    "- $p=1$: Manhattan distance\n",
    "- $p=2$: Euclidean distance\n",
    "- $p=\\infty$: Chebyshev distance\n",
    "\n",
    "Note that there are many more distance measures possible. It depends on the\n",
    "problem, which distance measure is most suitable. For example, the [cosine\n",
    "similarity](https://en.wikipedia.org/wiki/Cosine_similarity) \n",
    "$s(x_A, x_B)$ is often used for text data.\n",
    "\n",
    "$$\\begin{align}\n",
    "s(x_A, x_B) &= \\frac{x_A \\cdot x_B}{||x_A|| \\cdot ||x_B||} = \\frac{\\sum_{i=1}^d x_{A,i}x_{B,i}}{\\sum_{i=1}^d x_{A,i}^2 \\cdot\\sum_{i=1}^d x_{B,i}^2}\\\\[2em]\n",
    "d(x_A, x_B) &= 1 - s(x_A, x_B)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Don't get overwhelmed by this last formula. The cosine similarity measures \n",
    "the cosine of the angle between two vectors. If the vectors point into \n",
    "the same direction, the cosine is 1. If they are orthogonal (rechtwinklig), \n",
    "the cosine is 0. If they point into opposite directions, the cosine is -1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 2    ###\n",
    "########################\n",
    "\n",
    "# Assume we have a point x_A = [1, -2, 3] in 3D space. A second point x_B is\n",
    "# computed from x_A by adding a displacement vector delta. The displacement\n",
    "# can be scaled by a factor. \n",
    "\n",
    "# Compute the Euclidean, Manhattan and Cosine distance between the two points\n",
    "# for factors 0., 0.1, 1.0 and 10.0.\n",
    "\n",
    "factor = 1.0\n",
    "delta = np.array([2, 1, -1])\n",
    "x_A = np.array([1, -2, 3])\n",
    "x_B = x_A + factor*delta\n",
    "\n",
    "d_euclidean = ...\n",
    "d_manhattan = ...\n",
    "d_cosine = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45605429",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Normalization**\n",
    "\n",
    "\n",
    "Note that in some cases, it can be helpful to **normalize the data** before \n",
    "using the kNN algorithm because kNN relies on distance measurements to find \n",
    "the nearest neighbors. If the data is not normalized, features with larger \n",
    "numerical ranges (e.g., income in thousands vs. age in years) will dominate \n",
    "the distance calculation, making the algorithm biased toward these features. \n",
    "Normalizing ensures that all features contribute equally to the distance \n",
    "metric, leading to a more balanced and accurate prediction.\n",
    "\n",
    "Several methods exist to normalize the data in scikit-learn. Here, we use the\n",
    "StandardScaler, which scales the data to have zero mean and unit variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a57b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 3    ###\n",
    "########################\n",
    "\n",
    "# How to normalize the iris dataset such that each feature has zero mean and \n",
    "# unit variance?\n",
    "\n",
    "# Hint: Use the StandardScaler from sklearn.preprocessing.\n",
    "\n",
    "X_scaled = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba256dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Decision boundaries**\n",
    "\n",
    "In a previous tutorial (on decision trees), we have used the function \n",
    "plot_decision_boundary() to visualize the decision boundary of a classifier \n",
    "using two features. Let's use this function again to visualize the decision\n",
    "boundary, this time of a kNN classifier. For visualization purposes, we will\n",
    "only use the first two features of the iris dataset.\n",
    "\n",
    "\n",
    "```python\n",
    "def plot_decision_boundary(clf, X, y, \n",
    "                          n_steps=1000, \n",
    "                          data=None, \n",
    "                          ax=None):\n",
    "    \"\"\"\n",
    "    Visualize the decision boundary of an arbitrary classifier.\n",
    "\n",
    "    clf:  The classifier to plot.\n",
    "    X:    The features of the dataset.\n",
    "    y:    The labels of the dataset.\n",
    "    n_steps: Parameter controlling the resolution of plot.\n",
    "    ax:   (optional) The axis to plot on. If None, a new figure is created.\n",
    "    data: (optional) Data structure provided by sklearn.datasets.load_iris().\n",
    "    \"\"\"\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa07723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml import plot_decision_boundary\n",
    "\n",
    "features = [\"sepal length (cm)\", \"sepal width (cm)\"]\n",
    "\n",
    "# Use the scaled version if you managed to solve the previous exercise:\n",
    "X_2d = X_scaled[features]\n",
    "# Otherwise, just use:\n",
    "# X_2d = X[features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 4    ###\n",
    "########################\n",
    "\n",
    "# Use the function plot_decision_boundary() to look how the decision boundary\n",
    "# changes when using different configurations of the KNeighborsClassifier:\n",
    "# Just play around with the parameters and see what happens.\n",
    "# - Different values for n_neighbors: (1, 5, 15, 100)\n",
    "# - Different values for the weights parameter (uniform, distance)\n",
    "# - Different types distance metrics (euclidean, manhattan, cosine)\n",
    "# - Data with our without scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af951bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **kNN for regression**\n",
    "\n",
    "Here we demonstrate how to use kNN for a simple univariate regression task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d6823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def fun(x):\n",
    "    \"\"\"This is the ground truth function we want to approximate.\"\"\"\n",
    "    return np.sin(2*np.pi*x) + 2.5*(x-1)**2\n",
    "\n",
    "def generate_data(n_samples=100, noise=0.1, fun=fun, seed=42):\n",
    "    \"\"\"Given the function fun, generate n_samples with noise.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    x = np.random.uniform(0, 2, n_samples)\n",
    "    y = fun(x) + np.random.randn(n_samples)*noise\n",
    "    # Reshape the feature vector. scikit-learn objects expect a 2D \n",
    "    # array for the features (even if it's just one feature).\n",
    "    x = x.reshape(-1, 1)\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = generate_data(n_samples=100, noise=0.2)\n",
    "x_test, y_test = generate_data(n_samples=40, noise=0.2)\n",
    "xx = np.linspace(0, 2, 100)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4.5))\n",
    "axes[0].plot(xx, fun(xx), label=\"Ground truth\", linestyle=\":\", color=\"k\")\n",
    "axes[0].scatter(x_train, y_train, label=\"Train data\", color=\"k\", s=10, alpha=0.5)\n",
    "axes[0].scatter(x_test, y_test, label=\"Test data\", color=\"C1\")\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"y\")\n",
    "axes[0].set_title(\"Data\", fontweight=\"bold\")\n",
    "axes[0].set_ylim(-1.2, 4.2)\n",
    "axes[0].legend()\n",
    "\n",
    "n_neighbors = [1, 5, 50]\n",
    "axes[1].plot(xx, fun(xx), label=\"Ground truth\", color=\"k\", linestyle=\":\")\n",
    "for n in n_neighbors:\n",
    "    clf = KNeighborsRegressor(n_neighbors=n)\n",
    "    clf.fit(x_train, y_train)\n",
    "    i_sort = np.argsort(x_test.flatten())\n",
    "    x_test = x_test[i_sort]\n",
    "    y_test = y_test[i_sort]\n",
    "    y_pred = clf.predict(x_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"n_neighbors = {n:2d}:  MSE = {mse:.3f}\")\n",
    "    axes[1].plot(x_test, y_pred, label=f\"n_neighbors={n}\", \n",
    "                 marker='o', markerfacecolor='none')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"y (predicted)\")\n",
    "axes[1].set_title(\"K-Nearest Neighbors Regression\", fontweight=\"bold\")\n",
    "axes[1].set_ylim(-1.2, 4.2)\n",
    "fig.tight_layout(pad=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93588ef3",
   "metadata": {},
   "source": [
    "Again, we see that the model complexity influences the performance.\n",
    "For small n_neighbors, the model is more complex and can capture more\n",
    "details in the data. On the downside, it is more prone to overfitting.\n",
    "For large n_neighbors, the model is simpler and less prone to overfitting,\n",
    "but it might not capture relevant details in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ml-hs24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
