{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "760c90f9",
   "metadata": {},
   "source": [
    "# **SW12: Imbalanced data**\n",
    "\n",
    "Imbalanced data occurs when the distribution of classes in a dataset is highly skewed, with one class significantly underrepresented compared to others. This imbalance can lead to biased model performance, where the model favors the majority class and fails to correctly predict or recognize the minority class. Addressing imbalanced data is crucial in applications like fraud detection, medical diagnosis, and rare event prediction to ensure reliable and equitable outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebcd841",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Setup**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Some Jupyter magic for nicer output\n",
    "%config InlineBackend.figure_formats = [\"svg\"]   # Enable vectorized graphics\n",
    "\n",
    "# Adjust the default settings for plots\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import ml\n",
    "ml.setup_plotting()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae75479",
   "metadata": {},
   "source": [
    "### **Dataset / Problem**\n",
    "\n",
    "In this tutorial, we will use the [adult dataset](https://www.openml.org/search?type=data&status=active&id=1590). It contains data about \n",
    "adult persons extracted from a 1994 Census database. The task is to \n",
    "determine whether a person makes over 50K a year or not.\n",
    "\n",
    "We can obtain the data from [OpenML](https://www.openml.org/), an open online scientific platform for machine learning that provides open data, open algorithms and other resources. scikit-learn offers a function fetch_openml() to automatically download data from OpenML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b0a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "df, y = fetch_openml(\"adult\", version=2, as_frame=True, return_X_y=True)\n",
    "\n",
    "# Drop columns that are not needed\n",
    "df = df.drop(columns=[\"fnlwgt\", \"education-num\"])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display absolute and relative class frequencies\n",
    "classes_count = y.value_counts()\n",
    "display(classes_count)\n",
    "display(classes_count / classes_count.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3152f4b",
   "metadata": {},
   "source": [
    "The data is already imbalanced (with a 1:3 ratio). But since we want to \n",
    "demonstrate the effect of imbalanced data in a more pronounced way, we\n",
    "will artificially reduce the number of positive samples. Here, we will\n",
    "use the imbalanced-learn library to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41cd239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â To better highlight the effect of learning from an imbalanced dataset, \n",
    "# we increase its ratio to 30:1:\n",
    "from imblearn.datasets import make_imbalance\n",
    "sampling_strategy = {classes_count.idxmin(): classes_count.max() // 30}\n",
    "df, y = make_imbalance(df, y, sampling_strategy=sampling_strategy)\n",
    "\n",
    "display(y.value_counts())\n",
    "display(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7d7abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the data contain any missing values?\n",
    "missing_values = df.isnull().sum()\n",
    "display(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece1d7a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Data preprocessing**\n",
    "\n",
    "In the following, we will construct a pipeline for preprocessing the data. \n",
    "Inspection of the data reveals that the dataset contains both numerical and \n",
    "categorical features, which we want to process differently. Conveniently, \n",
    "scikit-learn provides the \n",
    "[ColumnTransformer](https://scikit-learn.org/stable/modules/compose.html#column-transformer),\n",
    "which allows us to apply different transformations to different columns of the input data.\n",
    "\n",
    "\n",
    "Note that the variables workclass, occupation, and native-country are \n",
    "sometimes missing. To handle these missing values, we will use the\n",
    "SimpleImputer class to fill in missing values. For numerical features,\n",
    "we will use the mean value, while for categorical features, we will use\n",
    "a constant value \"missing\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee0a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "# Pipeline for numeric data: Scale and impute missing values\n",
    "num_pipe = make_pipeline(\n",
    "    StandardScaler(), \n",
    "    SimpleImputer(strategy=\"mean\", add_indicator=True)\n",
    ")\n",
    "\n",
    "# Pipeline for categorical data: One-hot encode and impute missing values\n",
    "cat_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"),\n",
    ")\n",
    "\n",
    "# Combine both pipelines to a single preprocessor\n",
    "preprocessor = make_column_transformer(\n",
    "    (num_pipe, selector(dtype_include=\"number\")),\n",
    "    (cat_pipe, selector(dtype_include=\"category\")),\n",
    "    n_jobs=2,\n",
    ")\n",
    "\n",
    "# Let's train the preprocessing, and apply the transformation immediately.\n",
    "df = preprocessor.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caebbb4f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Baseline models**\n",
    "\n",
    "In the remainder of this tutorial, we will use the following function to train\n",
    "and evaluate the performance of a classifier. Besides accuracy and some other\n",
    "metrics, we will also report the balanced accuracy, which is defined as the\n",
    "average of recall obtained on each class. Balanced accuracy is equivalent to \n",
    "the arithmetic mean of specificity and sensitivity. This is metric particularly \n",
    "useful for imbalanced datasets.\n",
    "\n",
    "Furthermore, we employ 5-fold cross-validation ([cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html))\n",
    "to train a classifier and evaluate its performance on multiple splits (n=5) of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d7420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from time import time\n",
    "\n",
    "def train_evaluate(clf, title=None):\n",
    "    start = time()\n",
    "    ret = cross_validate(clf, df, y, cv=5,\n",
    "                         scoring=[\"accuracy\", \n",
    "                                  \"balanced_accuracy\",\n",
    "                                  #\"sensitivity\", \n",
    "                                  #\"specificity\",\n",
    "                                  ])\n",
    "\n",
    "    if title is not None:\n",
    "        print(title)\n",
    "        print(\"=\"*(len(title)+1))\n",
    "        \n",
    "    print(\"  accuracy=%.2f\" % ret['test_accuracy'].mean())\n",
    "    print(\"  balanced accuracy=%.2f\" % ret['test_balanced_accuracy'].mean())\n",
    "    print(\"  training time=%.2fs\" % (time() - start))\n",
    "    #print(\"  sensitivity=%.2f\" % ret['test_sensitivity'].mean())\n",
    "    #print(\"  specificity=%.2f\" % ret['test_specificity'].mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36592d88",
   "metadata": {},
   "source": [
    "### **DummyDummyClassifier**\n",
    "\n",
    "We will compare the performance of the different classifiers.\n",
    " We start with a dummy classifier that always predicts the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab32c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "train_evaluate(clf, title=\"DummyClassifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbc6bfc",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ Even though the accuracy is high, the balanced accuracy (i.e. the average of the sensitivity and specificity) is low.\n",
    "This indicates that the Dummy Classifier is not learning anything useful from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a18e48",
   "metadata": {},
   "source": [
    "### **Logistic regression**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0db702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "train_evaluate(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b75b87",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ The linear model is learning slightly better, but it is impacted by the class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b2c377",
   "metadata": {},
   "source": [
    "### **Random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7da271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42, n_jobs=2)\n",
    "train_evaluate(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119427e",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ This looks better. But still suffers from the inbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb15a603",
   "metadata": {},
   "source": [
    "### **Class weighting**\n",
    "\n",
    "Some classifiers allow to set the class weights to counteract the imbalance.\n",
    "The \"balanced\" mode uses the values of y to automatically adjust weights \n",
    "inversely proportional to class frequencies in the input data as:\n",
    "\n",
    "```python\n",
    "n_samples / (n_classes * np.bincount(y))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699cc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LR with Event-Weighting:\")\n",
    "clf = LogisticRegression(class_weight=\"balanced\", max_iter=1000)\n",
    "train_evaluate(clf)\n",
    "\n",
    "print(\"\\nRF with Event-Weighting:\")\n",
    "clf = RandomForestClassifier(class_weight=\"balanced\", random_state=42, n_jobs=2)\n",
    "train_evaluate(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0279acee",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ Class weighting helps with LR but not with RF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da6dbd",
   "metadata": {},
   "source": [
    "### **Specialized RandomForest from imbalanced-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cae4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "clf = BalancedRandomForestClassifier(random_state=42, n_jobs=2, \n",
    "                                     replacement=True,\n",
    "                                     bootstrap=False,\n",
    "                                     sampling_strategy=\"auto\")\n",
    "train_evaluate(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca1896a",
   "metadata": {},
   "source": [
    "### **Undersampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd12e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "df, y = rus.fit_resample(df,y)\n",
    "\n",
    "print(\"Undersampling + LR:\")\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "train_evaluate(clf)\n",
    "\n",
    "print()\n",
    "print(\"Undersampling + RF:\")\n",
    "clf = RandomForestClassifier(random_state=42, n_jobs=2)\n",
    "train_evaluate(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016e3928",
   "metadata": {},
   "source": [
    "### **Oversampling**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af9319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "df, y = ros.fit_resample(df,y)\n",
    "\n",
    "print(\"Oversampling + LR:\")\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "train_evaluate(clf)\n",
    "\n",
    "print()\n",
    "print(\"Oversampling + RF:\")\n",
    "clf = RandomForestClassifier(random_state=42, n_jobs=2)\n",
    "train_evaluate(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56c22f",
   "metadata": {},
   "source": [
    "## **SMOTE**\n",
    "\n",
    "Use [SMOTE (Synthetic Minority Over-sampling Technique)](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html) to generate synthetic samples\n",
    "for the minority class. SMOTE interpolates between existing samples to create new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e07697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "df, y = smote.fit_resample(df,y)\n",
    "\n",
    "print(\"SMOTE + LR:\")\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "train_evaluate(clf)\n",
    "\n",
    "print()\n",
    "print(\"SMOTE + RF:\")\n",
    "clf = RandomForestClassifier(random_state=42, n_jobs=2)\n",
    "train_evaluate(clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ml-hs24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
