{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067ed245",
   "metadata": {},
   "source": [
    "# **SW02: Curse of dimensionality**\n",
    "\n",
    "The curse of dimensionality refers to the fact that the feature space becomes **increasingly sparse** (dünn besetzt) as the number of dimensions increases. \n",
    "This sparsity makes it difficult for machine learning algorithms to find patterns in the data. In this notebook, we will explore the \n",
    "curse of dimensionality using a simple example.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85738798-ffe6-4c91-b3b0-204441ce83d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Some Jupyter magic for nicer output\n",
    "%config InlineBackend.figure_formats = [\"svg\"]   # Enable vectorized graphics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eed61925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify some global plotting parameters.\n",
    "import matplotlib as mpl\n",
    "#mpl.rcParams['figure.figsize'] = (5, 4)\n",
    "#mpl.rcParams['figure.dpi'] = 300\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "mpl.rcParams['axes.titlesize'] = 18\n",
    "mpl.rcParams['axes.titleweight'] = 'bold'\n",
    "#mpl.rcParams['axes.labelsize'] = 8\n",
    "mpl.rcParams['axes.labelweight'] = \"bold\"\n",
    "mpl.rcParams['grid.linestyle'] = '-'\n",
    "mpl.rcParams['grid.alpha'] = 0.4\n",
    "mpl.rcParams['legend.facecolor'] = [1,1,1]\n",
    "mpl.rcParams['legend.framealpha'] = 0.75\n",
    "mpl.rcParams['lines.linewidth'] = 1\n",
    "mpl.rcParams['xtick.top'] = True\n",
    "mpl.rcParams['ytick.right'] = True\n",
    "\n",
    "# Deafult line width: 2\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "\n",
    "# Initialize random number generator\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c4255",
   "metadata": {},
   "source": [
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d0594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function will be used to create $d$-dimensional data points randomly.\n",
    "# In each of the dimensions, the data points will be drawn from a uniform distribuiton.\n",
    "\n",
    "def sample_data(d=10, n=100):\n",
    "    return np.random.uniform(0, 1, size=(n, d))\n",
    "\n",
    "n = 1000\n",
    "d = 2\n",
    "X = sample_data(d=d, n=n)\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(X[:, 0], X[:, 1], s=10)\n",
    "plt.axis(\"square\")\n",
    "plt.title(\"Sample of 2D data (n=%d)\" % n);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c577d4b",
   "metadata": {},
   "source": [
    "Now, let's count the number of data points that are close to the border of the sample space $[0, 1]^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ea5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_border_points(X, margin=0.01):\n",
    "    \"\"\"Count the number of points that are close (within a margin) to \n",
    "    the border of the sample space [0, 1]^d.\"\"\"\n",
    "    return np.any((X < margin) | (X > 1 - margin), axis=1).sum()\n",
    "\n",
    "d = 2\n",
    "n = 1000000\n",
    "X = sample_data(d=d, n=n)\n",
    "margin = 0.01\n",
    "ratio = count_border_points(X=X, margin=margin) / n\n",
    "\n",
    "print(\"Fraction of border points for d=2: %.3f\" % ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c719c9ec",
   "metadata": {},
   "source": [
    "Not very surprisingly, and in correspondence with the above plot, the fraction of points close to the border is relatively small. As a matter of fact, for a margin of 0.01, we expect (1 - 0.98·0.98) = 0.04 of the points to be close to the border.\n",
    "\n",
    "Now, let's see how this fraction changes with the dimensionality of the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed38e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000            # Number of samples\n",
    "margin = 0.01       # Margin for border points\n",
    "\n",
    "ds = np.linspace(1, 1000, 101, dtype=int)\n",
    "fractions = np.zeros(len(ds))\n",
    "\n",
    "for i, d in enumerate(ds):\n",
    "    X = sample_data(d=d, n=n)\n",
    "    fractions[i] = count_border_points(X=X, margin=margin) / n\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"Fraction of border points for d=%4d: %.3f\" % (d, fractions[i]))\n",
    "    \n",
    "plt.figure()\n",
    "plt.title(\"Fraction of border points\")\n",
    "plt.xlabel(\"Number of dimensions\")\n",
    "plt.ylabel(\"Fraction of border points\")\n",
    "plt.stackplot(ds, fractions, 1-fractions, \n",
    "              labels=[\"Border points\", \"Inner points\"], alpha=0.5)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.02, 1));\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe9c96",
   "metadata": {},
   "source": [
    "Wow, this is counterintuitive! The fraction of border points increases with the number of dimensions! The higher the dimensionality, the more likely it is that a randomly drawn point is close to the border of the sample space. This implies that the local structure of the data is not very informative in high-dimensional data as all points are more or less equally far away from each other.\n",
    "\n",
    "Let's verify this last statement. In the following, we will create 1000 points `X1` and 1000 points `X2` in $d$ dimensions. Then, we compute the pairwise Euclidean distances between all points. To normalize the distances, we divide by the square root of the dimensionality $d$. (The diagonal of a $d$ dimensional unit cube has length $\\sqrt{1^2 + 1^2 + \\ldots} = \\sqrt{d}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pairwise_distances(X1, X2):\n",
    "    return np.sqrt(((X1 - X2)**2).sum(axis=1))\n",
    "\n",
    "ds = [1, 2, 5, 10, 50]\n",
    "n = 10000\n",
    "distances = np.zeros(len(ds))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.title(\"Distribution of pairwise distances\")\n",
    "\n",
    "palette = plt.get_cmap(\"Pastel1\")\n",
    "Ds = []\n",
    "\n",
    "for i,d in enumerate(ds):\n",
    "    X1 = sample_data(d=d, n=n)\n",
    "    X2 = sample_data(d=d, n=n)\n",
    "    # Compute pairwise distances, normalize by sqrt(d) for better comparison\n",
    "    # (don't mind the magic constant 6, it's just for scaling)\n",
    "    D = compute_pairwise_distances(X1, X2) / np.sqrt(d / 6)\n",
    "    ax.hist(D, bins=np.linspace(0, 2.5, 50), alpha=0.8, label=\"d=%d\" % d, \n",
    "            color=palette(i),\n",
    "            density=True,\n",
    "            edgecolor=\"none\")\n",
    "\n",
    "plt.legend()    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34903d7f",
   "metadata": {},
   "source": [
    "The histogram illustrates the distribution of pairwise distances between randomly drawn\n",
    "data points for different numbers of dimensions. It can be seen that the distributions\n",
    "become narrower as the number of dimensions increases, while the mean distance converges\n",
    "to a constant value. This means that in high-dimensional spaces, the distances between\n",
    "neighboring data points become more uniform, which makes it more difficult to discriminate\n",
    "between them. This is known as the **curse of dimensionality**.\n",
    "\n",
    "The curse of diemnsionality has several consequences:\n",
    "- It becomes increasingly difficult to discriminate between data points as the number of dimensions increases, because the distances between them become more uniform.\n",
    "- The density of data points decreases exponentially with the number of dimensions. \n",
    "- The number of data points required to obtain a representative sample grows exponentially with the number of dimensions.\n",
    "- The number of parameters required to model the data grows exponentially with the number of dimensions, which makes the models more complex and prone to overfitting.\n",
    "- The computational complexity of many algorithms grows exponentially with the number of dimensions, which makes them infeasible for high-dimensional data.\n",
    "\n",
    "Note that the above observations remain valid for distributions other than the uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "627a6622",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 1    ###\n",
    "########################\n",
    "\n",
    "# Imagine you have a dataset with 1000 samples and 20 features. Assuming that \n",
    "# the data is uniformly distributed in the unit hypercube [0, 1]^20, what is\n",
    "# the expected number of samples that are close to the border of the hypercube \n",
    "# (within a margin of 0.01)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33ca9c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 2    ###\n",
    "########################\n",
    "\n",
    "# To improve the performance of your machine learning model, you consider to\n",
    "# add more features to your dataset. The rationale is that more features will\n",
    "# allow the model to learn more complex patterns. What is the problem with this\n",
    "# approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82919f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 3    ###\n",
    "########################\n",
    "\n",
    "# The above analysis made idealized assumptions. Which ones? Are these \n",
    "# assumptions realistic in practice? And what if the assumptions are not \n",
    "# realistic?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
