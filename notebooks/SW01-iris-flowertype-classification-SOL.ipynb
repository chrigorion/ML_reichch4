{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067ed245",
   "metadata": {},
   "source": [
    "# **SW01: An introductory example**\n",
    "\n",
    "The iris dataset is a classic in machine learning and is often used in introductory tutorials to demonstrate (multi-class) classification.\n",
    "The dataset includes data on three classes of iris flowers: Iris setosa, Iris versicolor, and Iris virginica. For each of the 150 samples \n",
    "in the dataset, four measurements (predictor variables, or features) are given: \n",
    "\n",
    "- sepal length (in cm)\n",
    "- sepal width,\n",
    "- petal length \n",
    "- petal width\n",
    "\n",
    "The goal is to predict the class of the iris flower based on these four measurements.\n",
    "\n",
    "In this tutorial we demonstrate how to use the scikit-learn library to train a simple classifier on this iris dataset. \n",
    "As we are in the first week, of course you cannot understand everything in the code. However, still try to grasp \n",
    "the steps that are being performed, as the structuring of a machine learning project is always similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85738798-ffe6-4c91-b3b0-204441ce83d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import scikit-learn functionality\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Some Jupyter magic for nicer output\n",
    "%config InlineBackend.figure_formats = [\"svg\"]   # Enable vectorized graphics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import ml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fcb50b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Load and inspect the dataset**\n",
    "\n",
    "The scikit-learn package comes with a few standard datasets, including the iris dataset.\n",
    "We load the data as Pandas DataFrame, as it is easier to display and manipulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e496813-686d-4488-8116-049e78685013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is stored in a dictionary-like object.\n",
    "iris = load_iris(as_frame=True)\n",
    "\n",
    "# We can extract the following relevant information from the dataset:\n",
    "X = iris.data                         # Feature matrix\n",
    "y = iris.target                       # Target vector (by index)\n",
    "y_names = iris.target_names           # Names of the classes\n",
    "\n",
    "# Display the feature matrix and the target vector\n",
    "display(X)\n",
    "\n",
    "# Display the target vector\n",
    "display(y)\n",
    "\n",
    "# Display the target vector names\n",
    "display(y_names)\n",
    "\n",
    "# Note: If you are not familiar with Pandas, you use the following\n",
    "# commands to convert the data to numpy arrays. However, in the rest\n",
    "# of the code, we will use the Pandas dataframes.\n",
    "# X = X.to_numpy()\n",
    "# y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e841761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target vector identifies the class of each sample as an integer, \n",
    "# which is not very informative. We can map the class indices to the\n",
    "# class names to make the output more readable.\n",
    "y_mapped = y.map(pd.Series(y_names))  \n",
    "print(y_mapped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92a0b343-e567-4de2-85f9-d3ad9b05c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 1    ###\n",
    "########################\n",
    "\n",
    "# a) How many samples do we have? How many features?\n",
    "# b) How many samples are there for each class?\n",
    "# c) How many classes are there in the target vector?\n",
    "\n",
    "# Question a)\n",
    "n_samples = ...\n",
    "n_features = ...\n",
    "\n",
    "# Question b)\n",
    "n_samples_per_class = ...\n",
    "\n",
    "# Question c)\n",
    "n_classes = ...\n",
    "\n",
    "# Bonus: Visualization?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb375f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    SOLUTION 1    ###\n",
    "########################\n",
    "\n",
    "# Question a)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# Question b)\n",
    "n_samples_per_class = y.value_counts()\n",
    "n_samples_per_class_rel = (n_samples_per_class / n_samples).round(2)\n",
    "\n",
    "# Question c)\n",
    "n_classes = y.nunique()\n",
    "\n",
    "print(f\"Number of samples: {n_samples}\")\n",
    "print(f\"Number of samples per class (absolute): {n_samples_per_class.to_dict()}\")\n",
    "print(f\"Number of samples per class (relative): {n_samples_per_class_rel.to_dict()}\")\n",
    "print(f\"Number of features: {n_features}\")\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "\n",
    "# Bonus: Let's visualize the distribution as a pie chart.\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(n_samples_per_class,  # Absolute number of samples per class\n",
    "       labels = y_names,    # Names of the classes\n",
    "       autopct='%1.2f%%')    # Display percentages\n",
    "plt.show()\n",
    "ml.save_figure(path=\"iris_distribution.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc2ba8b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Visualize the data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a12a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data using violin plots.\n",
    "fig, axes = plt.subplots(2, 2, figsize=(9, 7))\n",
    "axes = axes.flatten()\n",
    "for i, feature in enumerate(iris.feature_names):\n",
    "    sns.violinplot(x=y_mapped,      # x: Stratify by class\n",
    "                   y=X[feature],    # y: Feature values\n",
    "                   hue=y_mapped,    # Color by class\n",
    "                   alpha=0.7,       # Transparency\n",
    "                   ax=axes[i])\n",
    "plt.tight_layout()\n",
    "ml.save_figure(path=\"iris_violin_plots.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e25be-298f-43cc-b541-bce6cfcb1338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data using histograms.\n",
    "fig, axes = plt.subplots(2, 2, figsize=(9, 7))\n",
    "axes = axes.flatten()\n",
    "for i, feature in enumerate(iris.feature_names):\n",
    "    sns.histplot(x=X[feature],      # x: Stratify by class\n",
    "                   #y=X[feature],    # y: Feature values\n",
    "                   hue=y_mapped,    # Color by class\n",
    "                   bins=25,\n",
    "                   stat=\"density\",\n",
    "                   alpha=0.3,       # Transparency\n",
    "                   #kde=True,\n",
    "                   ax=axes[i])\n",
    "    sns.kdeplot(x=X[feature], hue=y_mapped, fill=True, ax=axes[i])\n",
    "    axes[i].set_ylabel(feature)\n",
    "plt.tight_layout()\n",
    "ml.save_figure(path=\"iris_histogrm_plots.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800ba950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplots also provide a good overview of the data, by showing \n",
    "# the pairwise relationships between features.\n",
    "Xy = X.copy()\n",
    "Xy[\"class\"] = y_mapped\n",
    "g = sns.pairplot(Xy, hue=\"class\", height=7/4, aspect=1);\n",
    "ml.save_figure(path=\"iris_pair_plot.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdff409",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Model training**\n",
    "\n",
    "In this demonstration, we will use a simple logistic regression model to classify the iris dataset.\n",
    "\n",
    "It is advisable to split the data into a training and a test set. The training set is used to train the model, while the test set is used to evaluate the model's performance on unseen data. In this demonstration, we use a split ratio of 3:1 for training and testing (i.e.: 75% of the samples will be used for training, and 25% for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28414596-d830-414d-8a88-0fa579462784",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_mapped, \n",
    "                                                    test_size = 0.25)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737eeac0-900f-40b0-809f-880ae324f7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logicitic regression using the training data.\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the classes of the test data.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model.\n",
    "print(\"##############\")\n",
    "print(\"Accuracy: %.2f\" % accuracy_score(y_pred, y_test))\n",
    "print(\"##############\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acd92e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    EXERCISE 2    ###\n",
    "########################\n",
    "\n",
    "# a) Identify the main steps in the code above.\n",
    "# b) Try to formulate in your own words what happens in each step.\n",
    "#     - What is the purpose of the train_test_split() function?\n",
    "#     - What happens in the fit() and predict() functions?\n",
    "#     - Can you guess how the accuracy metric is defined?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5d14bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###    SOLUTION 2    ###\n",
    "########################\n",
    "\n",
    "# a) See the discussion section below.\n",
    "# b) - train_test_split(): \n",
    "#           - Shuffle the rows of (X, y) to have the samples in a random order. \n",
    "#             (Row vector X[i,:] and label y[i] correspond to the same sample.)\n",
    "#           - Use the first 75% (n_train=112) of the samples for training, and\n",
    "#             the remaining 25% (n_test=38) for testing.\n",
    "#           - This yields the training and test sets (X_train, y_train) \n",
    "#             and (X_test, y_test).\n",
    "#    - fit(): In this step, the model is trained using the training data.\n",
    "#             Using a an assumption about the functional relation between the\n",
    "#             features X and the target y, the model adjusts its parameters to\n",
    "#             minimize the error (measured by some loss function) on the \n",
    "#             training data. This involves an optimization problem. In the\n",
    "#             case of logistic regression, the optimal model parameters can be\n",
    "#             found analytically.\n",
    "#     - predict(): \n",
    "#             The model uses the learned parameters to predict the class of the\n",
    "#             test samples. We just call f(X_test|params), using the learned\n",
    "#             parameters.\n",
    "#     - accuracy_score(): \n",
    "#             This function compares the predicted classes with the true \n",
    "#             classes and computes the fraction of correctly predicted samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee609ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Discussion**\n",
    "\n",
    "Using just four predictors, we were able to predict the class of the iris flowers with an accuracy of 97%, \n",
    "which is a fairly good result, given the simplicity of the model and the amount of effort we put into it.\n",
    "\n",
    "Furthermore, we discovered that training and validating a logistic regression model with scikit-learn requires only five lines of code.\n",
    "The relevant functions were:\n",
    "- [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html): Split the data into training and test sets.\n",
    "- [`LogisticRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html): Create the estimator object (a logistic regression model).\n",
    "- [`fit()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit): Train the model using the training data.\n",
    "- [`predict()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict): Predict the classes of the test data.\n",
    "- [`accuracy_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html): Evaluate the accuracy of the model.\n",
    "\n",
    "\n",
    "### **Estimators**\n",
    "\n",
    "One of the central components in scikit-learn are the estimators. An estimator is any object that learns from data. This includes models for classification, regression, clustering, and more. Estimator objects in scikit-learn have a consistent interface. For instance, all estimators implement the `fit()` method, which is used to learn the model from\n",
    "the training data. The below list shows the main methods that are implemented by estimators:\n",
    "\n",
    "- `model.fit()`: Train the model using training data. For supervised learning applications,\n",
    "  this accepts two arguments: the feature matrix `X` and the labels `y`: `model.fit(X, y)`\n",
    "- `model.predict()`: Given a trained model, predict the label of a new set of data.\n",
    "  This method accepts one argument, the data `X_new` for which the labels are to be predicted:  `y_pred = model.predict(X_new)``\n",
    "- `model.predict_proba()`: Some classification estimators also provide this method, which \n",
    "  returns for each class the estimated probability that a sample belongs to that class. The output is a matrix where the `i-th` row corresponds to the sample `i` and the `j-th` column corresponds to the class `j`.\n",
    "  In this case (multi-class logistic regression), `model.predict()` returns the label with the highest probability.\n",
    "- `model.transform()`: This method is not implemented by all estimators, and does not play a role in the case of classifiers. \n",
    " It is used for data transformation methods, such as dimensionality reduction, feature selection, or feature extraction. \n",
    " It accepts one argument `X_new`, and returns the new representation of the data: `X_trans = model.transform(X_new)`\n",
    "- `model.fit_transform()`: Some transformation estimators implement this method,\n",
    "  if they require some of the transformation parameters to be learned from data.\n",
    "  It is a more efficient way to performs a fit and a transform on the same input data at once.\n",
    "\n",
    "For a complete list of sklearn terms see here: https://scikit-learn.org/stable/glossary.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
