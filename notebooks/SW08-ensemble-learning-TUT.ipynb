{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "913f08f1-c66e-4a72-8fec-d7b4a4ffaafe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **SW08: Ensemble learning**\n",
    "\n",
    "Ensemble learning is a technique in machine learning where multiple models, called \"learners\" or \"base models,\" are combined to solve the same problem. Instead of relying on just one model, ensemble learning takes advantage of the strengths of multiple models to improve accuracy and robustness. There are several types of ensemble methods, such as bagging (e.g., random forests), where models are trained independently and their results are averaged, and boosting, where models are trained sequentially to correct previous errors. By combining models, ensemble learning can produce a final prediction that is generally more accurate and less prone to errors than any single model.\n",
    "\n",
    "scikit-learn offers several means to create own ensembles of classifiers, which \n",
    "we will explore in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5d223a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1fc718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Some Jupyter magic for nicer output\n",
    "%config InlineBackend.figure_formats = [\"svg\"]   # Enable vectorized graphics\n",
    "\n",
    "# Adjust the default settings for plots\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import ml\n",
    "ml.setup_plotting()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d25036a",
   "metadata": {},
   "source": [
    "This time, we will use the [digits dataset](https://scikit-learn.org/1.5/modules/generated/sklearn.datasets.load_digits.html).\n",
    "The dataset contains 8x8 grayscale pixel images of handwritten digits, flattened to 64 pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8efdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_digits\n",
    "data = load_digits(as_frame=True) \n",
    "X, y = data.data, data.target\n",
    "\n",
    "display(X)\n",
    "\n",
    "# Let's visualize some of the images\n",
    "fig, axes = plt.subplots(3, 10, figsize=(9, 4.5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X.iloc[i].values.reshape(8, 8), cmap=\"gray\")\n",
    "    ax.set_title(f\"Digit {y[i]}\")\n",
    "    ax.axis(\"off\")\n",
    "fig.tight_layout()\n",
    "\n",
    "# To learn more about the dataset...\n",
    "# print(data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd1c5c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Majority voting**\n",
    "\n",
    "Let's build our own ensemble of learners using scikit-learn's \n",
    "[VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)\n",
    "\n",
    "The VotingClassifier is a meta-estimator that fits multiple base learners on \n",
    "the same dataset and combines their predictions using a voting strategy.\n",
    "Two voting strategies are available: hard voting and soft voting.\n",
    "- *Hard voting*: Equivalent to majority voting; the predicted class label is \n",
    "the majority class label predicted by all the classifiers.\n",
    "- *Soft voting*: Takes the average of the predicted probabilities for each\n",
    "class across all models. The final prediction is then the class with the \n",
    "highest average probability.\n",
    "\n",
    "Note that ties lead to a bias in the result, so avoid them if possible.\n",
    "For binary classification, it is advisable to choose an odd number of \n",
    "classifiers when doing hard voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4901cc33-566c-4a47-9ace-41f03cef4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some relevant imports\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad3c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_LR = LogisticRegression() \n",
    "clf_kNN = KNeighborsClassifier() \n",
    "clf_DT = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# We specify the classifiers as a list of tuples, where each tuple contains a\n",
    "# name for the classifier and the classifier itself.\n",
    "estimators=[('LR', clf_LR), ('kNN', clf_kNN), ('DT', clf_DT)]\n",
    "\n",
    "clf_voting = VotingClassifier(\n",
    "    estimators=estimators,\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Split the dataset into training set and test set\n",
    "X_train , X_test , y_train , y_test = train_test_split(X, y, test_size=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a31810-fca9-463a-b984-d457a83be3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we have seen in other notebooks, it is often helpful to standardize the \n",
    "# data before training. (Especially for kNN and LR are sensitive to the scale\n",
    "# of the features.)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b43d82-8252-428b-9d3b-0f022dd1658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train and predict the voting ensemble:\n",
    "clf_voting.fit(X_train, y_train)\n",
    "y_pred = clf_voting.predict(X_test)\n",
    "\n",
    "# Compute the prediction performance. How often is the classifier correct?\n",
    "print(\"Accuracy Vote: %.3f\" % accuracy_score(y_test , y_pred))\n",
    "\n",
    "#Â How do the single estimators perform?\n",
    "for clf_name, clf in estimators:\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Accuracy %4s: %.3f\" % (clf_name,accuracy_score(y_test , y_pred)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7390e49-7bd8-4605-9e15-44f95b248623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare the classifier's performance with hard and soft voting.\n",
    "for setting in ['soft','hard']: \n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=estimators,\n",
    "        voting=setting\n",
    "    )\n",
    "\n",
    "    voting_clf.fit(X_train,y_train)\n",
    "    y_pred = voting_clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test , y_pred)\n",
    "\n",
    "    # Model Accuracy, how often is the classifier correct?\n",
    "    print(\"Accuracy Vote (%s): %.3f\" % (setting, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf93daf-8fc4-49c5-921c-f5ea52f1238d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Bagging classifier**\n",
    "\n",
    "A bagging classifier is an ensemble method that trains multiple instances of the same base model on different random subsets of the training data, created through bootstrapping (sampling with replacement). Each model independently predicts the target, and the final prediction is made by aggregating the results, typically through majority voting (for classification) or averaging (for regression). This approach helps reduce overfitting and variance, making the overall model more stable and accurate compared to using a single model.\n",
    "\n",
    "The corresponing scikit-learn object is the [BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html).\n",
    "\n",
    "For this method, we will use one single base classifier. Here we employ a\n",
    "$k$-nearest neighbors classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9950af79-8915-4989-8b53-fdeb47b52781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "bagging = BaggingClassifier(KNeighborsClassifier(),\n",
    "                            random_state=42,  # For reproducibility\n",
    "                            max_samples=0.5,  # Use 50% of the samples per estimator\n",
    "                            max_features=0.3) # Use 30% of the features per estimator\n",
    "\n",
    "bagging.fit(X_train,y_train)\n",
    "y_pred = bagging.predict(X_test)\n",
    "print(\"Accuracy Bagging: %.3f\" % accuracy_score(y_test , y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9df338",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Boosting**\n",
    "\n",
    "Boosting is an ensemble learning technique that combines multiple weak learners (typically simple models, like shallow decision trees) sequentially to create a strong predictive model. In boosting, each model is trained to correct the errors of the previous ones by giving more weight to misclassified instances, so the ensemble focuses on difficult cases over time. Popular boosting algorithms, such as AdaBoost and Gradient Boosting, iteratively improve accuracy, resulting in a model that is both more accurate and resilient to errors compared to any individual learner in the sequence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc6ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (AdaBoostClassifier, GradientBoostingClassifier)\n",
    "\n",
    "clf_ada = AdaBoostClassifier(n_estimators=200, algorithm=\"SAMME\", \n",
    "                             random_state=42)\n",
    "clf_boost = GradientBoostingClassifier(n_estimators=300,   # Use 100 estimators\n",
    "                                       max_depth=3,       # Limit the depth of the trees\n",
    "                                       random_state=42)   # Use 100 estimators\n",
    "\n",
    "for key, clf in dict(Ada=clf_ada, Boost=clf_boost).items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Accuracy %-5s: %.3f\" % (key, accuracy_score(y_test , y_pred)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ml-hs24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
